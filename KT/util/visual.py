import numpy as np
import torch
from PIL import Image
import matplotlib.pyplot as plt


def plot_heatmap(matrix, title='Heatmap Example'):
    # Plot the heatmap
    plt.imshow(matrix, cmap='viridis', interpolation='nearest')
    plt.colorbar()  # Add a colorbar to show the scale
    plt.title(title)
    plt.show()


def plot_multiple_heatmap(matrix_list, title='Multiple Heatmap Example'):
    # Create subplots with 1 row and 3 columns
    n_matrix = len(matrix_list)
    fig, axes = plt.subplots(1, n_matrix, figsize=(n_matrix * 5, 5))

    if len(matrix_list) == 1:
        plot_heatmap(matrix_list[0])
    else:
        for i in range(n_matrix):
            matrix_i = matrix_list[i]
            # Plot the first matrix
            im_i = axes[i].imshow(matrix_i, cmap='viridis', origin='lower')
            axes[0].set_title(f'{title} {i}')

            # Add colorbars
            fig.colorbar(im_i, ax=axes[i])

        # Adjust layout for better spacing
        plt.tight_layout()

        # Show the plot
        plt.show()


def plot_multiplayer_radar_chart(categories, players_data, player_names, title='Radar Chart'):
    num_players = len(players_data)

    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))

    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

    for i in range(num_players):
        values = np.concatenate((players_data[i], [players_data[i][0]]))
        ax.fill(angles, values, alpha=0.25)
        ax.plot(angles, values, label=player_names[i], linewidth=2)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_title(title, size=15, y=1.1)

    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    plt.savefig('radar_chart.png', bbox_inches='tight')
    plt.show()


def visualize_word_embeddings_tsne(word_embeddings, perplexity=30, random_state=42):
    from sklearn.manifold import TSNE
    """
    Visualize word embeddings using t-SNE.

    Parameters:
    - word_embeddings (dict): A dictionary where keys are words and values are their corresponding embeddings.
    - perplexity (int): Perplexity parameter for t-SNE.
    - random_state (int): Random seed for reproducibility.

    Returns:
    - None (displays the plot).
    """

    # Extract words and corresponding embeddings
    words = list(word_embeddings.keys())
    embeddings = np.array([word_embeddings[word] for word in words])
    print(embeddings)
    # Apply t-SNE
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)
    embeddings_2d = tsne.fit_transform(embeddings)

    # Plot the results
    plt.figure(figsize=(10, 8))
    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])

    # Annotate points with words
    for i, word in enumerate(words):
        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))

    plt.title('t-SNE Visualization of Word Embeddings')
    plt.show()


import matplotlib.pyplot as plt
import numpy as np


def plot_model_performance(model_names, train_epochs, auc_scores_list):
    """
    Plot the performance chart for multiple deep learning models on the same plot.

    Parameters:
    - model_names: List of model names.
    - train_epochs: List of training epochs.
    - auc_scores_list: List of lists containing AUC scores for each model.

    Returns:
    - None (displays the plot).
    """
    print(model_names)
    print(train_epochs)
    plt.figure(figsize=(10, 6))

    for i, model_name in enumerate(model_names):
        print(len(train_epochs))
        print(len(auc_scores_list[i]))
        plt.plot(train_epochs, auc_scores_list[i], linestyle='-', label=model_name)
    dataset = 'Junyi'
    plt.title(f"Model Performance Comparison on {dataset} Dataset")
    plt.xlabel("Training Epochs")
    plt.ylabel("AUC Score")
    plt.legend()
    plt.grid(True)
    plt.show()


def test_tsne():
    import nltk
    from nltk.tokenize import word_tokenize
    nltk.download('punkt')
    import random
    def extract_words_from_document(document_path):
        with open(document_path, 'r', encoding='utf-8') as file:
            document_text = file.read()

        # Tokenize the words using nltk
        words = word_tokenize(document_text)

        return words

    # Replace 'your_document.txt' with the path to your actual document
    document_path = r'C:\Users\12574\Desktop\KT-Refine\nlp_data\text.txt'
    document_words = extract_words_from_document(document_path)

    print(document_words)

    document_words = random.sample(document_words, 100)
    from torchtext.vocab import GloVe

    # Specify the GloVe vectors you want to use (e.g., 6B for 6 billion tokens, 100d for 100-dimensional vectors)
    glove_vectors = GloVe(name='6B', dim=100)

    # Example: Get the embedding for a specific word (e.g., 'king')
    word_embedding = glove_vectors['king']

    print(f"Embedding for 'king':\n{word_embedding}")
    print(f"Embedding dimension: {len(word_embedding)}")

    random_words = random.sample(glove_vectors.stoi.keys(), 100)
    word_embeddings = {word: glove_vectors[word].numpy() for word in document_words}
    visualize_word_embeddings_tsne(word_embeddings)
    assert 1 == 1


def wtf_radar_chart():
    # Example data for two players
    categories = ['Category A', 'Category B', 'Category C', 'Category D', 'Category E']
    player1_data = [4, 3, 5, 2, 4]
    player2_data = [3, 4, 2, 5, 3]
    player_names = ['Player 1', 'Player 2']

    # Plot multiplayer radar chart
    plot_multiplayer_radar_chart(categories, [player1_data, player2_data], player_names,
                                 title='Multiplayer Radar Chart')


if __name__ == '__main__':
    # Generate more authentic data for testing
    model_names = ["HGAKT(MyKT)", "GKT","DKT", "GIKT", ]

    # Hypothetical AUC scores increasing over epochs
    auc_scores_model_a = [0.6104388625959325, 0.7098561536913373, 0.730985055505362, 0.740846395338746,
                          0.7475869736318423, 0.7527425312804067, 0.7571202150828891, 0.7607125772769666,
                          0.7639868901034632, 0.7668955508804504, 0.7695094564332078, 0.7719217185990621,
                          0.7740397548668204, 0.7759214834659387, 0.7776974686511987, 0.7793492519450423,
                          0.7807596687946109, 0.7820708008467927, 0.7833815048519518, 0.7846267949628628,
                          0.7857736694802465, 0.7868625209961786, 0.7878736563611047, 0.7888550919160665,
                          0.7897722691070388, 0.7906043703949099, 0.791480651091867, 0.7922458936293107,
                          0.7930532001575794, 0.7937700571431203, 0.7944490865082459, 0.7950416111235191,
                          0.7957131075564672, 0.7962404330877887, 0.7968699064276272, 0.7975072776993763,
                          0.7980573540072423, 0.7986901557369079, 0.7992088140874543, 0.7997404045156785,
                          0.8002217067303695, 0.8007877953189954, 0.8011980616262558, 0.8017354594193662,
                          0.8021746214130171, 0.8026517226564877, 0.8031862659608845, 0.8035775233507813,
                          0.8039043819389192, 0.8044151551223513, 0.804809743784978, 0.8053716546926042,
                          0.8057101582770572, 0.8061701653355835, 0.8064403222403003, 0.806928806555268,
                          0.807339233032735, 0.8077866058991137, 0.8081388771617712, 0.8085016382617313,
                          0.808804244706675, 0.8092846371228134, 0.8096087834643755, 0.8099951846576997,
                          0.8103294865126868, 0.8106980970184915, 0.8109360489943821, 0.8113938113259761,
                          0.8115393102036145, 0.8119457787478264, 0.8121479038940715, 0.8125531394787072,
                          0.8127907433807022, 0.8131536480565865, 0.813071146990724, 0.8134058977292518,
                          0.8139506509724319, 0.8141179961812244, 0.8143440473642055, 0.8144738707513087,
                          0.8148762970783507, 0.8154408625683175, 0.8154324949150176, 0.8159152342028352,
                          0.8159209955466902, 0.8163955382350103, 0.8166812683970641, 0.8171202300310135,
                          0.8174808400184183, 0.8176706928485621, 0.817981956239802, 0.8181987776848655,
                          0.8185300750228778, 0.8188132589411865, 0.8189269999692355, 0.8192730149340705,
                          0.8194724735955394, 0.8193073404340147, 0.8191383780994076, 0.8194466880530878]
    auc_scores_model_b = [0.66479945, 0.61875325, 0.5924158, 0.5753518, 0.56372243, 0.55534756, 0.54905045, 0.5440584,
                          0.53993255, 0.5364151, 0.5333578, 0.53064024, 0.528196, 0.5259832, 0.5239703, 0.52210045,
                          0.5203692, 0.5187817, 0.51728565, 0.5158794, 0.51456577, 0.51334095, 0.51215917, 0.5110541,
                          0.50999445, 0.5090538, 0.5081133, 0.50724447, 0.50640714, 0.5055927, 0.50481635, 0.50410074,
                          0.50336593, 0.5027919, 0.5020538, 0.5013797, 0.50073683, 0.50015396, 0.4996225, 0.4990462,
                          0.49850798, 0.4979237, 0.49747494, 0.49692562, 0.49639297, 0.49589583, 0.49538687, 0.49492842,
                          0.49454764, 0.4939569, 0.49353823, 0.49300718, 0.4925593, 0.49216455, 0.49179527, 0.49124977,
                          0.49078473, 0.49032068, 0.48992854, 0.48952317, 0.48916206, 0.48869377, 0.4883423, 0.4878928,
                          0.4875571, 0.4871713, 0.48686415, 0.48648518, 0.48625338, 0.48584118, 0.48557866, 0.48513773,
                          0.48483655, 0.4844221, 0.48448548, 0.4841558, 0.48355561, 0.48336017, 0.4831641, 0.4829975,
                          0.48255566, 0.48202485, 0.481957, 0.481427, 0.4813911, 0.4810396, 0.48068804, 0.4801677,
                          0.47978687, 0.4795136, 0.4791931, 0.4789727, 0.47858426, 0.47826147, 0.47809818, 0.4777163,
                          0.47756058, 0.4777137, 0.47778913, 0.47761688]
    auc_scores_model_c = [0.6672072987555252, 0.7049054933183447, 0.7174278723550255, 0.7250114631595741,
                          0.7307882691711981, 0.7352700574164268, 0.7387742455375623, 0.7419543499525637,
                          0.7448264583349679, 0.747280787652108, 0.7495647416608854, 0.7516404591219121,
                          0.7535492298662985, 0.7552826722222789, 0.7569423453674973, 0.7584708446997375,
                          0.7597947366793499, 0.7609658943622002, 0.7620101827298016, 0.7629255982892774,
                          0.7637963861594317, 0.7645429734149958, 0.7652686684435599, 0.7660229729552911,
                          0.7665749925706405, 0.7672208182162441, 0.7677929162767667, 0.7682770098462915,
                          0.7687630484190197, 0.7691105721171451, 0.7696958451473036, 0.7702807958784367,
                          0.7706342796287629, 0.7708361655062232, 0.7706667436298625, 0.7712344537593478,
                          0.7715274283032855, 0.7717744533519117, 0.7721491323170094, 0.7723038235424411,
                          0.772368804315385, 0.7728210090765726, 0.7730588739900741, 0.7732303430101544,
                          0.7735418740611668, 0.7736506447420313, 0.7738049253377918, 0.7740010138072909,
                          0.774207455699918, 0.7743799488949684, 0.7744901343973466, 0.7746142364270784,
                          0.7747723852279883, 0.7749747003957421, 0.7751003358494981, 0.7751406282126356,
                          0.7753420217179622, 0.775362826807694, 0.7755219190952647, 0.7754304225339212,
                          0.7758342400044826, 0.7755792196781739, 0.7757131012364417, 0.7758626095156608,
                          0.7760639485862542, 0.7759399843769317, 0.7763633905568796, 0.7761288863427203,
                          0.776232511275878, 0.7764470026006549, 0.7767264470111518, 0.7767348653366172,
                          0.7764745013817714, 0.7766766024065375, 0.7768292854616818, 0.7763501096080515,
                          0.7765471534678399, 0.7759539791021831, 0.7765218471521368, 0.7766221036495715,
                          0.7768542583502608, 0.7768765982582987, 0.7771212746399014, 0.7768996625273655,
                          0.7770115148625235, 0.7770483320213434, 0.7772372416225408, 0.7774020182517326,
                          0.777319598703524, 0.7774413275543668, 0.7771460104798541, 0.7773432158903977,
                          0.7772599835354517, 0.7772246071120833, 0.7773138085643625, 0.7773862345828614,
                          0.7768749291525973, 0.7768347708247616, 0.7771017235611003, 0.7770426189313516]
    auc_scores_model_d = [0.64305943, 0.6110705, 0.5903508, 0.5762792, 0.56641793, 0.55925274, 0.5538239, 0.5495382,
                          0.5460365, 0.54315484, 0.54070616, 0.5385447, 0.5366187, 0.53487986, 0.5332768, 0.5317759,
                          0.5304691, 0.5292988, 0.52824503, 0.5272737, 0.52640396, 0.525582, 0.5247851, 0.5240658,
                          0.5234102, 0.5227692, 0.5221728, 0.52164453, 0.521173, 0.52070194, 0.52017206, 0.51966023,
                          0.5192578, 0.5189462, 0.5186104, 0.51825535, 0.51795757, 0.51768565, 0.51734626, 0.5170737,
                          0.5168904, 0.516584, 0.51637715, 0.5160787, 0.5159373, 0.51574004, 0.515586, 0.5153018,
                          0.5150664, 0.5148794, 0.51485306, 0.5146491, 0.51463467, 0.5144201, 0.51430064, 0.51415855,
                          0.51399374, 0.5139679, 0.5137765, 0.51385695, 0.5136099, 0.51355106, 0.513614, 0.51343495,
                          0.5132767, 0.51327777, 0.5130255, 0.5133057, 0.51314896, 0.51298386, 0.5127745, 0.51269156,
                          0.51294017, 0.5129872, 0.51271325, 0.51293385, 0.51288617, 0.5138968, 0.51324993, 0.5128453,
                          0.51300657, 0.5129032, 0.5126508, 0.51275396, 0.5126328, 0.51261765, 0.51233333, 0.5121537,
                          0.51245844, 0.512219, 0.51287025, 0.5123728, 0.51248753, 0.5127393, 0.5124233, 0.51241523,
                          0.5133023, 0.5133665, 0.5129435, 0.5127297]
    auc_scores_model_e = [0.6646211272443259, 0.6999321892637972, 0.7077160983005079, 0.7150153378779055,
                          0.7188213395804106, 0.7216479645173446, 0.7237217527995893, 0.7252212084256896,
                          0.7265207494992209, 0.7278682451023522, 0.7297420078302793, 0.7313336916960843,
                          0.7320003033011052, 0.7328908973358906, 0.7341618037298754, 0.7350697542730444,
                          0.7358971630711226, 0.7374695834130941, 0.7379338793007831, 0.7386045517450843,
                          0.7386572311348653, 0.7385808978299626, 0.7389517431648907, 0.7392833602251303,
                          0.7391858898770167, 0.7400573742018837, 0.7403803818205434, 0.7410581543684244,
                          0.7412339181239751, 0.7415827248970911, 0.7414725446503339, 0.7415028964184164,
                          0.7418189803966245, 0.7428190368769793, 0.7428275462005933, 0.7431031421876232,
                          0.7432569848511674, 0.7434286470336241, 0.7433356824309902, 0.7437397376824819,
                          0.7433167161171101, 0.7433190319591346, 0.7438336688917829, 0.7440953035120863,
                          0.7439947751691881, 0.7439788787207233, 0.7439479963204756, 0.7447418323738298,
                          0.7448121717346508, 0.7448453538100198, 0.7447092118505209, 0.7453450971803619,
                          0.7450628716914294, 0.7451298279358911, 0.7456040981079843, 0.7453983723514691,
                          0.7454200193073476, 0.7450983974338988, 0.7454222790220377, 0.7455695378464786,
                          0.7455360055753592, 0.7457804914296106, 0.7457690105508745, 0.7460284830566698,
                          0.7465921053380548, 0.7463953797437827, 0.7471468728819509, 0.746549955074544,
                          0.7464598260955234, 0.7461883366143717, 0.7466819832576912, 0.7465553862637686,
                          0.7459587644123358, 0.7460299372796172, 0.7458953826924959, 0.7465512233021369,
                          0.7463701930373767, 0.7460830814548222, 0.7468669347911268, 0.7464575565711947,
                          0.7465960334219807, 0.7467586607219582, 0.7469085854388359, 0.7468616160604695,
                          0.7464099442400668, 0.7465603328744914, 0.7467049219046129, 0.7473833077730625,
                          0.7477361122184477, 0.7472935081240195, 0.7475704462852789, 0.7468871077050083,
                          0.7469071283015574, 0.7468722039125026, 0.7475936468031265, 0.7469974086817373,
                          0.7466621498969789, 0.7463705645900928, 0.7461810648751269, 0.7459644238813051]
    auc_scores_model_f = [0.5955590256058065, 0.6887647942366425, 0.7056241118821611, 0.7126327008876789,
                          0.7185045338379029, 0.7222153974663682, 0.7257795308213676, 0.7283509420313572,
                          0.729520716664422,
                          0.7311728926882095, 0.7326059483694731, 0.7346029710705245, 0.7360595637067431,
                          0.7371120153520287, 0.7383138398715057, 0.7391471533676018, 0.7398736195347231,
                          0.7408727066570122, 0.7418049558203247, 0.7427651054823469, 0.7436078309383845,
                          0.7443849263242157, 0.7449139480758972, 0.745480921852611, 0.746015596049526,
                          0.7467233643094057,
                          0.7473663219030854, 0.7475338507642699, 0.7480375494617989, 0.7484220106713558,
                          0.7487923651805365, 0.7492991671880684, 0.7496208690404386, 0.7503080580845076,
                          0.7506870385805396, 0.7511638917115208, 0.75155261999344, 0.7520348227570035,
                          0.7522799856660747,
                          0.7527476977719721, 0.7529392492777099, 0.7533227083900201, 0.7536531947447219,
                          0.7538311880018942, 0.754357955766967, 0.7545613987404773, 0.754719191829567,
                          0.7552306060339674,
                          0.7552310287957736, 0.7556246679522107, 0.7558001359115685, 0.7562318246828519,
                          0.7563349903929539, 0.7565609666618734, 0.7567423079349775, 0.7570185497195435,
                          0.7572075284090266, 0.7574487941472308, 0.7578830385718608, 0.7582332435015127,
                          0.7583472803909925, 0.7586242143379801, 0.7586160713329269, 0.7588933427251369,
                          0.7590579255582357, 0.7590665621928313, 0.7593055294326786, 0.7595528433644942,
                          0.7598082003221125, 0.7599888359649704, 0.7601274126913532, 0.7604384685913606,
                          0.760536645014206,
                          0.7607037276680333, 0.7606046686888813, 0.7608163517336006, 0.7612379039596757,
                          0.7616515948763681, 0.7618066575377329, 0.7619060529403471, 0.7619047351801269,
                          0.7623478682944813, 0.762175675426293, 0.7626451302813166, 0.7627942023175733,
                          0.7626497737629738,
                          0.7630735742719126, 0.7632447536001402, 0.7634410333892788, 0.7638090992960982,
                          0.7638379449017199, 0.7639492060788714, 0.7640369031541082, 0.7641669620660195,
                          0.7643826197236608, 0.7646620064218489, 0.7646718716574549, 0.7645190958959199,
                          0.7647331397722323, 0.7646520573916008]

metric_list = [auc_scores_model_a, auc_scores_model_c, auc_scores_model_e,auc_scores_model_f]
common_seq_len = 9999
for m in metric_list:
    print(m)
    common_seq_len = min(len(m), common_seq_len)
    print(common_seq_len)
align_metric_list = [m[:common_seq_len] for m in metric_list]
epochs = np.arange(0, common_seq_len)
# Plot the performance chart for all models on the same plot
plot_model_performance(model_names, epochs, align_metric_list)
